= SDD 0013 - SYNsights Metrics and Alerting

:sdd_author:    Marco Fretz
:sdd_owner:     Marco Fretz
:sdd_reviewers: SYN Workgroup
:sdd_date:      2019-10-25
:sdd_status:    draft
include::partial$meta-info-table.adoc[]

[NOTE]
.Summary
====
Collect metrics from different part of the infrastructure, platform, platform services and applications or managed services running on the platform. These metrics can later be used for alerting and dashboards.

Prometheus is the de-facto-standard to collect metrics especially in the kubernetes ecosystem. Rancher managed kubernetes cluster and OpenShift Container Platform already bring Prometheus for their node and kubernetes monitoring. It's therefore obvious that we'll also use Prometheus for our monitoring as this allows us to make use of the https://prometheus.io/docs/prometheus/latest/federation/[Prometheus Fedaration] concept.
====

== Motivation

Having the correct metrics helps to create useful and actionable alerts. To debug problems and incidents it's key to have all relevant metrics shown on easy to understand dashboards. Ideally we can also correlate logs of all relevant containers with these metrics. This design aims to reuse tools and concepts which are already around and "standard" in the kubernetes world instead of reinventing the wheel.

=== Goals

* In a metrics system based on a time series database, have all relevant metrics, this includes
** Node metrics
** Everything that the underlying cluster montioring system already provides
** metrics from new VSHN managed services are automatically collected as soon such a service is provisioned
** metrics from all platform services (e.g. monitoring tools, GitOps, etc) are automatically collected
* Provide the framework for other services to have their metrics scraped and define alerting rules

=== Non-Goals

* Dashboards
* Logging
* Monitoring kubernetes and nodes directly

== Design Proposal

Depending on what is already brought by the kubernetes cluster management (Rancher, OCP) we install our Prometheus in a different namespace "vshn-monitoring" and make our Prometheus federate with the cluster monitoring Prometheus, e.g. the one that comes from Rancher and sits in the "cattle-prometheus" namespace.

When scraping a federation endpoint of another Prometheus we can filter which metrics we want. This helps to limit the amount of data.

Other (Managed) Services, which are https://wiki.vshn.net/pages/viewpage.action?pageId=151914965[crossplane] (consumable managed services) or https://wiki.vshn.net/pages/viewpage.action?pageId=151914962[comodore] (platform services) provisioned, can register itself in the same Prometheus to have their metrics scraped.

Then we have all metrics in one place, so we can easily use node, kubernets and application / service metrics to build useful alerts (and later in dashboards correlate all metrics).

=== Operator

In the kubernetes world Prometheus is usually managed using the https://github.com/coreos/prometheus-operator[Prometheus Operator]. Rancher already installs this operator for us ready to use. On non-rancher platform the configuration management decides whether we install the operator on our own. The configuration management also decides with which (and how) we federate with the the cluster monitoring prometheus.

==== Provisioning

Creating a Prometheus instances is then done by creating a customer resource of `kind: Prometheus`

[source,yaml]
--
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  labels:
    prometheus: vshn-monitoring
  name: prometheus
  namespace: vshn-monitoring
spec:
  alerting:
    alertmanagers:
    - name: alertmanager-main
      namespace: vshn-monitoring
      port: web
  baseImage: quay.io/prometheus/prometheus
  nodeSelector:
    kubernetes.io/os: linux
  podMonitorSelector: {}
  replicas: 1
  resources:
    requests:
      memory: 400Mi
  ruleSelector:
    matchLabels:
      prometheus: k8s
      role: alert-rules
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: vshn-prometheus-sa
  serviceMonitorNamespaceSelector:
    matchLabels:
      VshnMonitoringID: main
  serviceMonitorSelector: {}
  version: v2.11.0
--

By using the `serviceMonitorNamespaceSelector` and `serviceMonitorSelector` we can use labels to limit from where we pull in scrape configs into this instance of prometheus.

Usually we want to filter this by using a label on the namespaces which we do monitoring for: Label `VshnMonitoringID: main` which means our main prometheus (there could be any number of VSHN and customer operated Prometheus instances on a platform).

The Rancher managed Prometheus already has good filters, so we don't have to care that it will pull in our scrape configs too.

==== Federation with cluster monitoring

This is done by specifying a scrape config and target using the `kind: ServiceMonitor. `

[source,yaml]
--
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: federated
  namespace: vshn-monitoring
spec:
  namespaceSelector:
    matchNames:
      - cattle-prometheus
  selector:
    matchLabels:
      app: prometheus
  endpoints:
  - port: http
    interval: 10s
    path: /federate
    params:
      'match[]':
      - '{namespace="cattle-prometheus"}'
    honorLabels: true
--

The match param is used to filter which metrics we want to scrape. By default we scrape everything that is scraped within the namespace of the prometheus cluster-monitoring on Rancher managed clusters.

It is yet to be defined what exactly we should scrape.

==== Scrape config and targets

Other scrape configs are also managed by using `kind: ServiceMonitor` in any namespace on the cluster.

For example to monitor the metrics of our deployed grafana (out of scope in this SDD):

[source,yaml]
--
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: grafana
  namespace: vshn-monitoring
spec:
  namespaceSelector:
    matchNames:
      - vshn-monitoring
  endpoints:
  - interval: 15s
    port: grafana-http
  selector:
    matchLabels:
      app: grafana
--

This means that any other service can bring a in `kind: ServiceMonitor` its https://wiki.vshn.net/pages/viewpage.action?pageId=151914962[comodore] component (this is comparable to using profile_icinga2::resources::check in the a Puppet Profile) to have its metrics scraped.

=== Pushing Metrics

synsight-metrics also manages a prometheus-push gateway which is automatically scraped by prometheus. Tools like k8up (https://wiki.vshn.net/pages/viewpage.action?pageId=153847068[SDD#2019-10-30 - Backup Framework]) or other jobs can push metrics to this gateway within the cluster.

=== Alerting

Alertmanager is configured using the Prometheus Operator objects, too. This is described in detail here: https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/alerting.md

* Alertmanager receivers and routes are configured in a Secret (required by the operator). The data of the secret is dynamically generated by the synsights-metrics component from hierarchical data (inventory). Any arbitrary config is possible this way. 
* It is possible that the customer configures its own Alertmanager using the operator, to configure very specific custom use-cases.

Alerting is done using Alert Rules on the metrics we collected. This is configured using `kind: PrometheusRule`.

This means that any other service can bring a in `kind: PrometheusRule` its https://wiki.vshn.net/pages/viewpage.action?pageId=151914962[comodore] component (this is comparable to using profile_icinga2::resources::check in the a Puppet Profile) to install the Alerting Rule.

=== Heartbeats (Monitoring of Monitoring)

We need to know, when a cluster monitoring is not working anymore and therefore not able to send alerts, e.g. Prometheus or Alertmanager are down or not functional. In that case, we would never notice that something is wrong with managed platform. 

This problem is solved using periodic (say every 60seconds) heartbeats to "VSHN central", where an alert is automatically created when heartbeats for a managed platform are missing for some time (say 300s).

* It is important that heartbeats are generated as normal alerts from normal prometheus rules and are send via the exact same alertmanager. only this way, we can ensure the heartbeat covers the full component chain.
* Scrape jobs of prometheus still need prometheus rules and alerting - but as prometheus itself and alertmanager are covered by the heartbeat we can use normal alerting here.

=== Requirements

* Cluster Monitoring is based on Prometheus
* Persistent Storage Class which is used to store Prometheus metrics persistently
* Prometheus Operator already installed in a usable version or able to install the Operator on our own

=== Risks and Mitigations

* We'll depend on some installed components that are out of our control:
** Prometheus managed by Rancher or OCP
** Prometheus Operator already installed by Rancher or OCP. There is a chance that we want a newer version for example.

== Drawbacks

* While using Prometheus with its Operator bring great automation it also limits us within the kubernetes cluster. It does not allow us to have cross-cluster monitoring or monitor services from external as we do in the Icinga2 world currently.  

== Alternatives

This was not part of the tool evaluation. Using Prometheus is seen as a given fact.

== References

* Prometheus configuration: https://prometheus.io/docs/introduction/overview/
* Prometheus operator: https://github.com/coreos/prometheus-operator
